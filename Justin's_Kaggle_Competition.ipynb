{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Justin's Kaggle Competition",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U4DAYzjE7zN"
      },
      "source": [
        "Hello world, \n",
        "\n",
        "This notebook will include a data cleaning and some machine learning methods implimented to find out an outcome of home sales prices based on a large group of features. First we will begin by loading in some useful packages (we may add some more along the road)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSP5WL-mKaB"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(seed=484)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpgUlSPCGU-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7427850f-4fe5-47a7-8517-762ece71948e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXyeIG6SIaYT"
      },
      "source": [
        "\n",
        "\n",
        "First to deal with our category of all NA.  The way we will deal with this issue is a bit funky and we will use the mapper function ahead to help make the data workable for our ML methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6my-xhTrqqw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "efe292f6-4d61-4dfb-8b18-9cbc82e78d93"
      },
      "source": [
        "# Funky function. \n",
        "def mapper(df1, df2):\n",
        "    df = pd.concat([df1,df2])\n",
        "    print(df.shape)\n",
        "    all_maps = {}\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == 'O':\n",
        "            maps = {v:i for i,v in enumerate(set(df[c]))}\n",
        "            df[c] = df[c].map(maps)\n",
        "            all_maps[c] = maps\n",
        "    df1 = df[df.index < 1460]\n",
        "    df2 = df[df.index >= 1460]\n",
        "    return df1,df2, all_maps\n",
        "\n",
        "train = mapper(train,test)[0]\n",
        "test = mapper(train,test)[1]\n",
        "print(train.shape,test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fced4ed18f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGMX2-scI1-2"
      },
      "source": [
        "Okay, so this mapper function changes the string categories or letters into numeric categories/numbers. Before I attempted using getdummies and dropping nas to accomplish this but the dimensions were somewhat problematic to work with so i resorted to this concat method where we concatinate or join the two dataframes together. We then split and index the training and test set by < 1460 and >=1460. Changing all of the Na values into numeric integers should make our ML methods workable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHMyuzB3NdqH"
      },
      "source": [
        "Testdata=pd.read_csv('/content/gdrive/MyDrive/Econ 484/test.csv', na_filter=False,index_col='Id')\n",
        "Traindata=pd.read_csv('/content/gdrive/MyDrive/Econ 484/train.csv', na_filter=False,index_col='Id')\n",
        "Testdata.head()\n",
        "Traindata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU7-k4OEJksc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIOhZiPiH543"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG2xdCksOh0x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekejGHlYIJNP"
      },
      "source": [
        "As we can see that we have A LOT OF FEATURES!!! The machine learning methods that will implemented such as Lasso will work to in essence throw out some of th emore useless features that have little explanatory power. We will also use Random forest in Ridge Regression,\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mivo3SB4H5ny"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9rNF-uxtMqX"
      },
      "source": [
        "# Here the outcome we are interested in is SalePrice and here we are using ALL OTHER VARIABLES as Covariates or features on our outcome.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12ugiOqKxpp2"
      },
      "source": [
        "# Here we are standardizing our data using standard scalar. \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "X_scaled = scaler.transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMvmDxvgw26z"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,random_state=42) # Here we split the data into test and training."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PEeo0A1xK6E"
      },
      "source": [
        "depth_grid = {'max_depth': [2,3,4,5,6,7,8,9,10,11,12,13]}\n",
        "grid_search = GridSearchCV(RandomForestRegressor(),depth_grid,cv=5,return_train_score=True)\n",
        "best_model=grid_search.fit(X_train,y_train)\n",
        "print(\"Best depth: \",best_model.best_estimator_.get_params()['max_depth'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RuVM3iRzPak"
      },
      "source": [
        "print(\"The accuracy of the Random Forest Regression is: \" + str(best_model.score(X_test,y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_p3gUqvMEq"
      },
      "source": [
        "# here we are predicting sales price on our test set\n",
        "\n",
        "Salesppredict = best_model.predict(Testdata)\n",
        "print(Salesppredict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-t1-uUuGHhl"
      },
      "source": [
        "print(x.shape)\n",
        "Testdata.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOz9Xh3WGHS8"
      },
      "source": [
        "# Repeating for other steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugh6MXEF0jK9"
      },
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import Ridge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2kBUug41Atg"
      },
      "source": [
        "# Here we cross validate \n",
        "\n",
        "ridgecv= RidgeCV(cv=5).fit(X_train, np.ravel(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bl2UFGMpTQg"
      },
      "source": [
        "# We can se ridge does good on the training set\n",
        "\n",
        "ridgecvalpha =Ridge(alpha = ridgecv.alpha_,max_iter=100000).fit(X_train,y_train)\n",
        "print(ridgecvalpha.score(X_train,y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RnfPSYipd9a"
      },
      "source": [
        "# It appears that Random forest performs better than ridge regression\n",
        "print('Ridge score on test set: {:.4f}'.format(ridgecvalpha.score(X_test,y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJZr0hLSpS6S"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fewNkDonqCBk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU85cDwHqCTI"
      },
      "source": [
        "# Now lets try Lasso\n",
        "\n",
        "from sklearn.linear_model import Lasso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99HjGOoTqF4N"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lasso = Lasso(alpha=0.002, max_iter=100000)\n",
        "scores = cross_val_score(lasso,X_train,y_train,cv=5)\n",
        "print(\"Average cross-validation score: {:.4f}\".format(scores.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xllEWHqsqZ8a"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# define grid for alpha\n",
        "alpha_grid = {'alpha': [.0001, .0005,.001, .002, .003, .004, .006, .008, .01, .012, .014, .016 ,.018, .02 ],'max_iter': [100000]}\n",
        "grid_search = GridSearchCV(Lasso(),alpha_grid,cv=5,return_train_score=True)\n",
        "best_model=grid_search.fit(X_train,y_train)\n",
        "print(\"Best alpha: \",best_model.best_estimator_.get_params()['alpha'])\n",
        " # Grid serch for lasso if used on midterm\n",
        "lasso = Lasso().fit(X_train,y_train)\n",
        "param_grid = {\"alpha\": [.0001, .0005,.001, .002, .0022, .003, .004, .006, .008, .01, .012, .014, .016 ,.018, .02 ], \"max_iter\": [100000]}\n",
        "lazzo = GridSearchCV(lasso,param_grid,cv=5,return_train_score=True)\n",
        "lazzo.fit(X_train,y_train)\n",
        "print(lazzo.best_params_, lazzo.best_score_, sep='\\n')\n",
        "lasso = Lasso(alpha=0.0022, max_iter=100000).fit(X_train,y_train)\n",
        "y_pred = lasso.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnj0WhpqqcOV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79QM2MHVx1B0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}